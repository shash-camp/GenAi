# üß† FEATURE EXTRACTION

In **NLP (Natural Language Processing)**, we must convert **text into numbers** (features) so that a computer or machine learning model can understand it. This is called **feature extraction**.

Earlier methods used simple ways like counting words or checking document length.  
Newer methods like **Word2Vec** and **GLoVe** give more meaningful word information using deep learning.

---

## üîπ What is Bag of Words?

**Bag of Words (BoW)** is a **text representation technique** used in NLP.  
It converts a **collection of text documents** into **numerical features** that a machine learning model can understand.

> It **does not care about grammar, word order, or meaning** ‚Äî just the **frequency** of each word in a document.

---

## üîπ How It Works (Simple Steps)

1. **Collect all unique words** in your text data (called the **vocabulary**).
2. For each document:
   - Count how many times each word from the vocabulary appears.
   - Represent this count as a **vector**.

---

## üì¶ Example

Let‚Äôs say we have two documents:

- **Doc1**: `"I love cricket"`
- **Doc2**: `"I play cricket and I love football"`

### Step 1: Vocabulary (unique words from both documents)


### Step 2: Vector Representation

| Word     | Doc1 | Doc2 |
|----------|------|------|
| I        | 1    | 2    |
| love     | 1    | 1    |
| cricket  | 1    | 1    |
| play     | 0    | 1    |
| and      | 0    | 1    |
| football | 0    | 1    |

- **Doc1** ‚Üí `[1, 1, 1, 0, 0, 0]`  
- **Doc2** ‚Üí `[2, 1, 1, 1, 1, 1]`

---

## ‚úÖ Advantages of Bag of Words

- ‚úÖ Very **simple** to understand and implement.
- ‚úÖ A **good baseline** for many NLP tasks (spam detection, sentiment analysis, etc.).
- ‚úÖ Works well with **small datasets**.

---

## ‚ùå Disadvantages of Bag of Words

| Problem                  | Description                                                                                         |
|--------------------------|-----------------------------------------------------------------------------------------------------|
| ‚ùå Ignores Context        | Doesn‚Äôt understand word meaning or order. `"I am not happy"` and `"I am happy"` may look similar.  |
| ‚ùå High Dimensionality    | Large vocabulary ‚Üí **sparse vectors** (lots of 0s).                                                 |
| ‚ùå Common Words Dominance | Words like ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúand‚Äù dominate the count but carry little actual meaning.                   |
| ‚ùå Poor for Similarity    | Doesn‚Äôt recognize that `"movie"` and `"film"` are related words.                                   |

---

## üí° How Later Techniques Solved These Issues

| Technique            | How It Helped                                                                                                                             |
|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
| **TF-IDF**           | Gave **higher importance to rare words**, reduced weight for common ones like ‚Äúthe‚Äù.                                                     |
| **n-grams**          | Considered **word combinations** (e.g., ‚Äúnot good‚Äù) to preserve some **context**.                                                        |
| **Word2Vec / GLoVe** | Created **dense word vectors** that capture **semantic meaning**. Similar words (e.g., "man", "boy") have similar vector values.          |

---

